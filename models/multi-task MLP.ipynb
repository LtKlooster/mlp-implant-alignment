{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35165e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "n_epochs = 300\n",
    "learning_rate = 0.05\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de964d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom train validation test set split for reproducing purposes\n",
    "def train_val_test_split(X, y, training_size, val_every=10, test_every=10, test_offset=5, seed=seed):\n",
    "    \"\"\"\n",
    "    Custom data split with absolute training size and fixed patterns for validation/test set.\n",
    "\n",
    "    Args:\n",
    "        X, y: torch tensors of equal length\n",
    "        training_size (int): number of training samples to include\n",
    "        val_every (int): every nth sample goes to validation set\n",
    "        test_every (int): every nth sample goes to test set\n",
    "        test_offset (int): offset to start test selection (e.g. every 10th starting at index 5)\n",
    "        seed (int): random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test (torch tensors)\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(X)\n",
    "    indices = list(range(n))\n",
    "\n",
    "    # Validation set: every nth sample starting at index 0\n",
    "    val_indices = list(range(0, n, val_every))\n",
    "    # Test set: every nth sample starting at offset\n",
    "    test_indices = list(range(test_offset, n, test_every))\n",
    "\n",
    "    # Remaining samples are potential training candidates\n",
    "    remaining_indices = [i for i in indices if i not in val_indices + test_indices]\n",
    "\n",
    "    # Shuffle for randomness\n",
    "    random.seed(seed)\n",
    "    random.shuffle(remaining_indices)\n",
    "\n",
    "    # Cap training size to available data\n",
    "    training_size = min(training_size, len(remaining_indices))\n",
    "    train_indices = remaining_indices[:training_size]\n",
    "\n",
    "    # Helper to slice tensors by indices\n",
    "    def select(tensor, idxs):\n",
    "        return tensor[idxs]\n",
    "\n",
    "    X_train, y_train = select(X, train_indices), select(y, train_indices)\n",
    "    X_val, y_val = select(X, val_indices), select(y, val_indices)\n",
    "    X_test, y_test = select(X, test_indices), select(y, test_indices)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47096802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('../dataset/dummy_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target columns\n",
    "X = df[[\"Fem_Fle(+)Ext(-)\", \"Fem_Var(+)Val(-)\", \"Fem_Int(+)Ext(-)\"]].values\n",
    "y = df[[\"EI\", \"VV\", \"LM\", \"AP\", \"PD\", \"PCL\", \"dMCL\", \"sMCL\", \"LCL\", \"ALL\", \"OPL\", \"PC\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Split into train/val/test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y, training_size=80)\n",
    "\n",
    "# Normalize y per target\n",
    "y_mean = y_train.mean(dim=0)   # shape (12,)\n",
    "y_std = y_train.std(dim=0)     # shape (12,)\n",
    "y_train = (y_train - y_mean) / y_std\n",
    "y_val   = (y_val   - y_mean) / y_std\n",
    "y_test  = (y_test  - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5348e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor datasets & dataloaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task MLP model\n",
    "class MultiTaskMLP(nn.Module):\n",
    "    def __init__(self, input_size, shared_hidden_sizes, task_hidden_size, target_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = input_size\n",
    "        for h in shared_hidden_sizes:\n",
    "            layers += [nn.Linear(in_dim, h), nn.LeakyReLU()]\n",
    "            in_dim = h\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, task_hidden_size),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(task_hidden_size, 1)\n",
    "            )\n",
    "            for _ in range(target_size)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.shared(x)\n",
    "        out = [head(feat) for head in self.heads]\n",
    "        return torch.cat(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "model = MultiTaskMLP(\n",
    "    input_size=3,\n",
    "    shared_hidden_sizes=[30, 20],\n",
    "    task_hidden_size=20,\n",
    "    target_size=12\n",
    ")\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d375f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = np.mean([\n",
    "                loss_fn(model(Xb), yb).item()\n",
    "                for Xb, yb in val_loader\n",
    "            ])\n",
    "        print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1348ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "model.eval()\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        y_true_all.append(y_batch)\n",
    "        y_pred_all.append(y_pred)\n",
    "\n",
    "# Denormalize\n",
    "y_true_all = torch.cat(y_true_all, dim=0)\n",
    "y_pred_all = torch.cat(y_pred_all, dim=0)\n",
    "y_true_all = y_true_all * y_std + y_mean\n",
    "y_pred_all = y_pred_all * y_std + y_mean\n",
    "\n",
    "# Compute metrics per target\n",
    "mse_per_target = ((y_true_all - y_pred_all) ** 2).mean(dim=0)\n",
    "mae_per_target = (y_true_all - y_pred_all).abs().mean(dim=0)\n",
    "print(\"Metrics per target:\")\n",
    "for i, col in enumerate([\"EI\",\"VV\",\"LM\",\"AP\",\"PD\",\"PCL\",\"dMCL\",\"sMCL\",\"LCL\",\"ALL\",\"OPL\",\"PC\"]):\n",
    "    print(f\"  {col:>5s} | MSE: {mse_per_target[i]:.6f} | MAE: {mae_per_target[i]:.6f}\")\n",
    "\n",
    "# Define which are kinematics and which are ligaments\n",
    "kinematic_cols = [\"EI\",\"VV\",\"LM\",\"AP\",\"PD\"]\n",
    "ligament_cols  = [\"PCL\",\"dMCL\",\"sMCL\",\"LCL\",\"ALL\",\"OPL\",\"PC\"]\n",
    "\n",
    "# Compute weighted total for each sample\n",
    "def compute_f(df_tensor):\n",
    "    df_np = df_tensor.numpy()\n",
    "    kin_sum = df_np[:, :len(kinematic_cols)].sum(axis=1)\n",
    "    lig_sum = df_np[:, len(kinematic_cols):].sum(axis=1)\n",
    "    return kin_sum + 40 * lig_sum  # apply your weighting\n",
    "\n",
    "# Compute metrics on weighted totals\n",
    "f_true = compute_f(y_true_all)\n",
    "f_pred = compute_f(y_pred_all)\n",
    "mse_f = np.mean((f_true - f_pred) ** 2)\n",
    "mae_f = np.mean(np.abs(f_true - f_pred))\n",
    "\n",
    "print(\"Metrics on weighted sums:\")\n",
    "print(f\"MSE: {mse_f:.6f}\")\n",
    "print(f\"MAE: {mae_f:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
